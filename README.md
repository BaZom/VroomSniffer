# ğŸš— VroomSniffer - Car Monitoring System

<div align="center">
  <img src="ui/resources/logo3.jpg" alt="VroomSniffer Logo" width="300">
</div>

> **ğŸ“ Note: This is a hobby project created for educational and personal use only.**

A modern, service-oriented web scraping system designed to collect car listings from various online marketplaces, detect new listings, and send notifications via Telegram. Built with Python, Playwright, and Streamlit.

## âœ¨ Key Features

- **ğŸ­ Modern Web Scraping**: Playwright-based engine handles JavaScript-heavy sites
- **ğŸ”„ Smart Deduplication**: Automatically detects and filters out duplicate listings
- **ğŸ“± Telegram Notifications**: Get instant alerts for new car listings with rich formatting
- **â±ï¸ Flexible Scheduling**: Configure custom intervals for automatic scraping
- **ğŸš€ Auto-Notifications**: Automatically send new listings as they're discovered
- **ğŸŒ Web Dashboard**: Interactive Streamlit interface with real-time monitoring
- **âš¡ CLI Interface**: Command-line tools for automation and scripting
- **ğŸ”§ Service-Oriented Architecture**: Clean separation of concerns with specialized services
- **ğŸ“Š Multiple Storage Options**: JSON-based storage with extensible service layer

## ğŸš€ Quick Start

### 1. Installation

**Clone the repository:**
```bash
git clone <your-repository-url>
cd car_scraper
```

**Create virtual environment:**
```bash
# Windows
python -m venv venv
.\venv\Scripts\Activate.ps1

# Linux/macOS  
python3 -m venv venv
source venv/bin/activate
```

**Install dependencies:**
```bash
pip install -r requirements.txt
playwright install
```

### 2. First Run

**Test the scraper:**
```bash
python cli/main.py run "https://www.example-marketplace.com/s-autos/bmw/k0c216"
```

**View results:**
```bash
python cli/main.py list
```

**Launch web interface:**
```bash
streamlit run ui/streamlit_app.py
```

## ğŸ“‹ Usage

### Command Line Interface

The CLI provides the primary interface for running the scraper and managing results:

```bash
# Run the scraper with a marketplace search URL
python cli/main.py run "https://marketplace-url.com/search-cars"

# Run with auto-notifications (sends new listings automatically)
python cli/main.py run "https://marketplace-url.com/search-cars" --notify --notify-count 3

# Schedule periodic scraping (every 60 seconds for 10 runs)
python cli/main.py schedule "https://marketplace-url.com/search-cars" --interval 60 --runs 10 --notify

# List the latest scraped listings
python cli/main.py list

# Search listings for specific keywords
python cli/main.py search "bmw x5"
python cli/main.py search "automatic"

# Send a listing via Telegram (use index from list command)
python cli/main.py send 3

# Send top 5 listings via Telegram
python cli/main.py send-top 5

# Send summary notification
python cli/main.py notify

# Show version information
python cli/main.py version

# Get help for any command
python cli/main.py --help
```

### Web Interface

Launch the interactive Streamlit dashboard for advanced filtering and monitoring:

```bash
streamlit run ui/streamlit_app.py
```

**Key Features:**
- **ğŸ”„ Flexible Auto-monitoring**: Set custom intervals from 30 seconds to 1 hour
- **ğŸ“² Auto-notifications**: Automatically send new listings to Telegram  
- **ğŸ›ï¸ Interactive controls**: Manual monitoring with one click
- **ğŸ“Š Real-time analytics**: Price trends and statistics
- **ğŸ” Advanced filtering**: Car make/model, price, year, transmission, mileage
- **ğŸ§  Smart URL Pool**: Manage multiple search URLs with automatic rotation

**Auto-Monitoring Setup:**
1. Configure your search filters or add search URLs to the pool
2. âœ… Set your desired interval (30s to 1h)
3. âœ… Click "Start Scraper" to begin automatic monitoring
4. âœ… Enable "Auto-send new listings" for Telegram notifications (optional)
5. New listings are instantly sent to your Telegram

The web interface provides:
- Real-time listing monitoring with progress visualization
- **ğŸ”„ Configurable auto-monitoring intervals**
- **ğŸ” URL pool management for multiple search queries**
- **ğŸ“² Auto-send new listings to Telegram**
- Advanced filtering options
- Price analysis and trends
- Visual data exploration
- Telegram integration

## Project Structure

```
car_scraper/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ cli/                     # Command-line interface
â”‚   â”œâ”€â”€ main.py             # Main CLI application
â”œâ”€â”€ ui/                     # Web interface
â”‚   â”œâ”€â”€ streamlit_app.py    # Streamlit web app
â”‚   â”œâ”€â”€ components/         # UI component modules
â”‚   â”‚   â”œâ”€â”€ error_handling.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ navigation.py
â”‚   â”‚   â”œâ”€â”€ scraper_controls.py
â”‚   â”‚   â”œâ”€â”€ sound_effects.py
â”‚   â”‚   â”œâ”€â”€ state_management.py
â”‚   â”‚   â”œâ”€â”€ styles.py
â”‚   â”‚   â”œâ”€â”€ telegram_controls.py
â”‚   â”‚   â”œâ”€â”€ ui_components.py
â”‚   â”‚   â””â”€â”€ url_management.py
â”‚   â”œâ”€â”€ pages/              # Page components for the UI
â”‚   â”‚   â”œâ”€â”€ scraper.py      # Main scraper page
â”‚   â”‚   â”œâ”€â”€ home.py         # Home page
â”‚   â”‚   â”œâ”€â”€ data_storage.py # Data management page
â”‚   â”‚   â””â”€â”€ playground.py   # Testing/playground page
â”‚   â””â”€â”€ resources/          # UI resources (images, sounds)
â”œâ”€â”€ providers/              # Service provider pattern implementation
â”‚   â””â”€â”€ services_provider.py # Centralized service factory/singleton manager
â”œâ”€â”€ scraper/                # Scraping engine
â”‚   â””â”€â”€ engine.py           # Main scraping engine
â”œâ”€â”€ services/               # Service layer (business logic)
â”‚   â”œâ”€â”€ storage_service.py       # Data storage operations
â”‚   â”œâ”€â”€ url_pool_service.py      # URL management
â”‚   â”œâ”€â”€ statistics_service.py    # Analytics and statistics
â”‚   â”œâ”€â”€ notification_service.py  # Notification handling
â”‚   â”œâ”€â”€ scraper_service.py       # Scraper operations
â”‚   â””â”€â”€ scheduler_service.py     # Scheduling and timing
â”œâ”€â”€ storage/                # Data persistence
â”‚   â”œâ”€â”€ db.py              # Database operations
â”‚   â”œâ”€â”€ latest_results.json      # Latest scraping results
â”‚   â”œâ”€â”€ latest_new_results.json  # New listings from last run
â”‚   â””â”€â”€ all_old_results.json     # Historical listings cache
â”œâ”€â”€ notifier/              # Notification system
â”‚   â””â”€â”€ telegram.py        # Telegram integration
â”œâ”€â”€ proxy/                 # Proxy management
â”‚   â””â”€â”€ manager.py
â”œâ”€â”€ utils/                 # Utilities
â”‚   â””â”€â”€ deduplication.py
â”œâ”€â”€ scheduler/             # Job scheduling
â”‚   â””â”€â”€ job.py
â”œâ”€â”€ config/                # Configuration
â”‚   â””â”€â”€ car_models.py
â””â”€â”€ logger/                # Logging
    â””â”€â”€ logging_config.py
```

### Core Components
- `cli/` â†’ **Command-line interface** with both ad-hoc and scheduled scraping
- `ui/` â†’ **Web interface** with real-time monitoring and URL pool management
- `providers/` â†’ **Service provider** implementation for dependency management
- `scraper/` â†’ **Scraping engine** using Playwright for JavaScript-heavy sites
- `services/` â†’ **Service layer** with specialized services for each concern
- `storage/` â†’ **Centralized data storage** using JSON files
- `notifier/` â†’ **Notifications** via Telegram

### Service Layer Architecture
- `providers/services_provider.py` â†’ Centralized service factory/singleton manager
- `services/storage_service.py` â†’ Handling data persistence and retrieval
- `services/url_pool_service.py` â†’ Managing search URLs and URL pools
- `services/statistics_service.py` â†’ Generating analytics and statistics
- `services/notification_service.py` â†’ Managing notification delivery
- `services/scraper_service.py` â†’ Coordinating scraping operations
- `services/scheduler_service.py` â†’ Managing timed scraping jobs
- `scheduler_service.py` â†’ Managing timing and scheduling

---

## Configuration

### Telegram Integration
Configure Telegram notifications for automatic car listing alerts:

**Setup Steps:**
1. Create a Telegram bot via @BotFather
2. Get your bot token and chat ID  
3. Configure environment variables in `.env`:
   ```env
   TELEGRAM_BOT_TOKEN=your_bot_token_here
   TELEGRAM_CHAT_ID=your_chat_id_here
   TELEGRAM_TEST_MODE=false  # Set to true for testing
   ```

**Auto-Notification Features:**
- **ğŸš€ Streamlit UI**: Toggle "Auto-send new listings" in sidebar for real-time notifications
- **âš¡ CLI Scraper**: Use `--notify` flag to auto-send listings after scraping
- **ğŸ“± Rich Formatting**: HTML messages with emojis, clickable links, and structured layout
- **ğŸ›¡ï¸ Rate Limiting**: Smart delays prevent spam and API limits
- **ğŸ”§ Test Mode**: Corporate-friendly mode for networks that block Telegram

### Proxy Support
Configure proxy rotation in `proxy/manager.py` for enhanced scraping reliability.

### Database Storage
- **SQLite**: Default lightweight option (configured in `storage/db.py`)
- **PostgreSQL**: Production-ready option for larger deployments

---

## Development

### Running Tests
```bash
python -m pytest tests/
```

### Project Architecture
The project follows a clean, modular architecture with separation of concerns:
- **CLI**: User interface and command handling
- **Scraper**: Web scraping logic using Playwright
- **Services**: Business logic and data processing
- **Storage**: Data persistence and management
- **UI**: Web-based dashboard and visualization
- **Utils**: Shared utilities and helpers

---

## Troubleshooting

- **Playwright issues**: Run `playwright install` to download browser binaries
- **Import errors**: Ensure you're in the virtual environment and all dependencies are installed
- **Scraping failures**: Check if the target website structure has changed
- **Telegram not working**: Verify bot token and chat ID in the notifier configuration

---

## âš ï¸ Disclaimer

**This is a hobby project created for educational and personal use only.**

- **Educational Purpose**: This project is intended for learning web scraping techniques and automation concepts
- **Personal Use**: Use this tool responsibly and only for personal research and learning
- **Respect Website Terms**: Always respect website terms of service and robots.txt files
- **Rate Limiting**: Implement appropriate delays and respect server resources
- **No Commercial Use**: This project is not intended for commercial or large-scale scraping operations
- **User Responsibility**: Users are responsible for ensuring their usage complies with applicable laws and regulations

---

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/your-feature`
3. Make your changes and add tests
4. Run tests: `python -m pytest tests/`
5. Commit your changes: `git commit -am 'Add some feature'`
6. Push to the branch: `git push origin feature/your-feature`
7. Submit a pull request

---

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Architecture

### Service Provider Pattern

This project uses the Service Provider pattern to manage service dependencies. The pattern is implemented in `providers/services_provider.py` and provides a centralized mechanism for accessing service instances throughout the application.

Key benefits:
- **Singleton services**: Only one instance of each service exists
- **Lazy initialization**: Services are only created when needed
- **Automatic dependency injection**: Services can depend on other services
- **Consistent state**: Both UI and CLI use the same service instances

Example usage:
```python
from providers.services_provider import get_storage_service, get_scraper_service

# Get service instances
storage_service = get_storage_service()
scraper_service = get_scraper_service()

# Use services
listings = storage_service.get_all_cached_listings(path)
scraper_service.run_scraper(filters)
```
